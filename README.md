# Expressive_Gesture 
üî•(ICME 2024) ExpGest: Expressive Speaker Generation Using Diffusion Model and Hybrid Audio-Text Guidance

**[Paper](https://arxiv.org/abs/2410.09396)**

> *IEEE International Conference on Multimedia and Expo (ICME), 2024*

This is the official repository of the ExpGest.

![image](https://github.com/cyk990422/ExpGesture/blob/main/9.png)

ExpGest is a method that accepts audio, phrases, and motion description text as inputs, and based on a diffusion model, it generates highly expressive motion speakers.



# News :triangular_flag_on_post:

- [2025/03/13] **Code release!** ‚≠ê
- [2024/10/12] **ExpGest is on [arXiv](https://arxiv.org/abs/2410.09396) now.**
- [2024/03/08] **ExpGest got accepted by ICME 2024!** üéâ
## Requirements

### Conda environments
```
conda create -n ExpGest python=3.7
conda activate ExpGest 
conda install -n ExpGest pytorch==1.10.0 torchvision==0.11.1 cudatoolkit=10.2 -c pytorch
pip install -r requirements.txt
```

### Pre-trained model and data
- Download [CLIP](https://drive.google.com/drive/folders/1CN9J2T1tN-F2R5qfHjOfMkGXP00oka6E?usp=drive_link) model. Put the folder in the root.
- Download pre-trained weights from [here](https://drive.google.com/drive/folders/1GNGsOKTJf6GSrp9OENi0AmA2UkrwLx7u?usp=drive_link) (only audio-control) and put it in `./mydiffusion_zeggs`
- Download pre-trained weights from [here](https://drive.google.com/drive/folders/175TyMLMjzXz5vkCHOmvB9v7YxXcs40Te?usp=drive_link) (action-audio-control) and put it in `./mydiffusion_zeggs`
- Download pre-trained weights from [here](https://drive.google.com/drive/folders/1_l3LMxYZvyWGjn9D9qQVdbPkmClDfI5K?usp=drive_link) (text-audio-control) and put it in `./mydiffusion_zeggs`
- Download WavLM weights from [here](https://drive.google.com/drive/folders/1du41ziM0utAMjCtn-YPM8ZYOI6YplHrq?usp=drive_link) and put it in `./mydiffusion_zeggs` 

## Demo üéâ

```
# More detailed controls such as texts and phrases, please set them in the configuration file.
# Run audio-control demo:
python sample_demo.py  --audiowavlm_path '../1_wayne_0_79_79.wav' --max_len 320 --config ../ExpGest_config_audio_only.yml

# Run hybrid control demo:
python sample_demo.py  --audiowavlm_path '../1_wayne_0_79_79.wav' --max_len 320 --config ../ExpGest_config_hybrid.yml

# Run demo with emotion guided:
python sample_demo.py  --audiowavlm_path '../1_wayne_0_79_79.wav' --max_len 320 --config ../ExpGest_config_hybrid_w_emo.yml
```

## Training code coming soon !




# For Beats datasets
Note: All FGD evaluation models are trained in a way similar to the trimodal model, using linear velocity representation as input. Among them, "body" refers to the evaluation using only all the torso joints from the upper body above the spine 3 to the left and right wrists in SMPLX.
"Full" represents the evaluation with an additional 30 finger joints.
"EA" stands for emotion - alignment. A sentiment classifier is trained using UNet. Its input is the generated motion sequence, and the output is a label corresponding to one of the 8 emotions in the Beat data.
The accuracy of this classifier on the speaker 1 sequence of the Beat data is 95.7%.
It is used to determine whether the motion sequences generated by our ExpGes conform to the emotional categories of the original data.


|method|FGD_full on raw|FGD_full on feature|FGD_body on raw|FGD_body on feature|EA body|EA hand|
|---|---|---|---|---|---|---|
|Ground Truth|0.0|0.0|0.0|0.0|0.97|0.96|
|CAMN|-|-|52.4|263.9|-|-|
|Trimodal|-|-|47.6|212.7|-|-|
|DiffuseStyleGesture|52.7|303.9|33.7|133.9|0.60|0.49|
|ExpGes w/o emo-guided|40.6|199.1|14.8|84.9|0.68|0.55|
|ExpGes|25.3|115.0|11.7|76.6|0.91|0.88|
|ExpGes + hybrid|44.6|226.1|25.4|129.3|0.89|0.82|

"EC" represents the success rate of emotion - control. Similarly, the trained sentiment classifier is used to perform gradient - guided emotion control. Then, the final motion sequences generated by the emotion - guidance module + ExpGes are subjected to emotion judgment to determine whether they conform to the artificially given emotions.

|method|EA body|EC body|EA hand|EC hand|
|---|---|---|---|---|
|DiffuseStyleGesture|0.60|0.27|0.49|0.19|
|DiffuseStyleGesture w emo-guided|0.83|0.69|0.70|0.63|
|ExpGes|0.91|0.83|0.81|0.70|



## Citation
```
@inproceedings{cheng2024expgest,
  title={ExpGest: Expressive Speaker Generation Using Diffusion Model and Hybrid Audio-Text Guidance},
  author={Cheng, Yongkang and Liang, Mingjiang and Huang, Shaoli and Ning, Jifeng and Liu, Wei},
  booktitle={2024 IEEE International Conference on Multimedia and Expo (ICME)},
  pages={1--6},
  year={2024},
  organization={IEEE}
}
```

## Acknowledgement
The pytorch implementation of MANO is based on [DiffuseStyleGesture](https://github.com/YoungSeng/DiffuseStyleGesture). We use some parts of the great code from [FreeTalker](https://github.com/YoungSeng/FreeTalker) and [MLD](https://github.com/ChenFengYe/motion-latent-diffusion). We thank all the authors for their impressive works!

## Contact
For technical questions, please contact cyk990422@gmail.com

